---
title: "Tobin Predoctoral Data Task"
subtitle: "Experiments to Enhance the Efficiency of Medicaid"
author: "Sheena Tan"

date: today

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    code-fold: false
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon="false"}
## Github Repo Link

<https://github.com/sheena-tan/tobintask.git>
:::

# Introduction

This project uses data provided by Yale's Tobin Center for Economic Policy as part of a data task for a pre-doctoral research position. For the first task, the data draws from a larger research project aiming to understand hospital response to reimbursement rate adjustment received as part of the Medicare Modernization Act (MMA) of 2003, and whether their adoption of medical technologies increased as a result of those adjustments. Task 2 uses sample Medicaid data to investigate the effects of managed care.

## Task 1

### Exercise 1

::: {.callout-note collapse="true" icon="false"}
## Code (for reproducibility)

```{r, eval = FALSE}
library(tidyverse)

# check missingness: n_missing = 0 for all variables
# check duplicates: n_unique = 1305 for both datasets
skimr::skim(mma_main)
skimr::skim(mma_treat)

# Clean primary key ----
mma_main_join <- mma_main |>
  mutate(prov_id = tolower(prov_id))

mma_treat_join <- mma_treat |>
  mutate(prov_id = tolower(prov_id))

# Join data ----
mma_data <- mma_treat_join |>
  left_join(mma_main_join)

```
:::

An initial inspection of the data revealed no missingness. The provided data files were joined using provider ID as a primary key, resulting in a merged dataset with 13050 unique observations of 38 variables.

### Exercise 2

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
mma_data |>
  group_by(year) |>
  mutate(prov_total = n_distinct(prov_id)) |>
  skimr::skim(prov_total)
# N_t each year [2001-2010] constant = 1305

# create intermediate index variable before sum for all `tech_n`
for (i in 1:31) {
  mma_saidin <- mma_saidin |>
    group_by(year) |>
    mutate(
      # number of hospitals with tech_n
      !!paste0("sum_", i) := sum(!!sym(paste0("tech_", i))), 
      # weight a_k,t
      !!paste0("weight_", i) := 1 - (1/1305) * !!sym(paste0("sum_", i)), 
      !!paste0("index_", i) := 
        !!sym(paste0("tech_", i)) * !!sym(paste0("weight_", i))
    )
}

# sum across `index_n` to find saidin index for each hospital per year
mma_saidin <- mma_saidin |>
  pivot_longer(cols = starts_with("index"), names_to = "index") |>
  group_by(prov_id, year) |>
  summarise(sum(value)) |>
  rename(saidin = `sum(value)`)

# join to merged data
mma_data <- mma_saidin |>
  left_join(mma_data)

```
:::

A variable `saidin` was constructed to represent the Saidin Index Score[^1] for all hospitals in each year and joined with the merged dataset to create the working dataset with **13050 observations** of **39 variables**. This index characterizes a hospital's range of available technologies, weighted for their rarity, and provides a way through which to investigate how hospital technology grows and changes, particularly in response to interventions.

[^1]: Baker, L. & Spetz, J. (1999). Managed care and medical technology growth. *Frontiers in Health Policy Research, 2*, 31.

### Exercise 3

#### Part A

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
library(psych)

mma_2004 <- mma_data |> 
  filter(year == 2004)

describe(mma_2004$saidin, IQR = TRUE, skew = FALSE)
```
:::

::: {.callout-note icon="false" appearance="simple"}
# Summary Statistics

```{r, echo = FALSE}
library(psych)
library(dplyr)
library(here)
load(here("task1/data/mma_data.rds"))
mma_2004 <- mma_data |> 
  filter(year == 2004)
describe(mma_2004$saidin, IQR = TRUE, skew = FALSE)
```
:::

The distribution of Saidin Index scores in 2004 is strongly right-skewed, with values ranging from **0 to 11.57**, a **median of 1.19**, and an **IQR of 1.84**.

![Density-boxplot of Saidin Index scores in 2004](task1/figures/saidin_2004.png){#fig-1}

As the Saidin Index is helpful for characterizing technology growth rates, the distributions by year can also be found below:

::: {.callout-tip collapse="true"}
## ADDITIONAL EXPLORATORY ANALYSIS

![Density ridge plot of Saidin Index scores by year](task1/figures/saidin_density_ridge.png){#fig-2}

![Stacked barplots of Saidin Index scores by year](task1/figures/saidin_boxplot.png){#fig-3}

A period of rapid relative growth looks to have occurred between 2002 and 2005 (@fig-3), starting in 2003 (@fig-2), the same year that the Medicare Modernization Act (MMA) was initiated. However, this rapid growth has since slowed, with growth between 2007 and 2010 becoming increasingly limited.

It is also important to note that the scores have not shifted uniformly; rather, their distribution has become more spread out, with a large number of outliers developing on the higher end (better technology availability). Should these outliers correlate with those hospitals receiving MMA treatment, the data would suggest ***support for the intervention's positive impact***.
:::

#### Part B

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
# create mutually exclusive type variable
mma_types <- mma_2004 |> 
  mutate(
    hospital_type = case_when(
      nonprof == 1 & govt == 0 ~ "nonprof",
      govt == 1 & nonprof == 0 ~ "govt",
      govt == 0 & nonprof == 0 ~ "neither",
      TRUE ~ NA_character_
    )
  )

# assuming normality due to sample size: one-way ANOVA
anova_types <- aov(saidin ~ hospital_type, data = mma_types)
summary(anova_types)

# Tukey's test
TukeyHSD(anova_types)
```
:::

![Density plots of Saidin scores by hospital type in 2004](task1/figures/plot_types_2004.png){#fig-4}

Since the analysis was limited to 2004, I operationalize "rates of technology adoption" as the Saidin scores for that year. An initial density plot of Saidin scores by hospital type (i.e., nonprofit, government, neither) visualizes **seemingly significant differences** between non-profit hospitals and both other types. Government hospitals and hospitals that are neither non-profit nor government do not seem to differ much from each other.

To investigate if technology adoption rates were significantly different between the hospital types, I conduct a one-way ANOVA with the null hypothesis that the true difference in Saidin scores between hospital types is zero.

::: {.callout-note icon="false" appearance="simple"}
# ANOVA Results

```{r, echo = FALSE}
mma_types <- mma_2004 |> 
  mutate(
    hospital_type = case_when(
      nonprof == 1 & govt == 0 ~ "nonprof",
      govt == 1 & nonprof == 0 ~ "govt",
      govt == 0 & nonprof == 0 ~ "neither",
      TRUE ~ NA_character_
    )
  )
anova_types <- aov(saidin ~ hospital_type, data = mma_types)
summary(anova_types)
```
:::

::: {.callout-note icon="false" appearance="simple"}
```{r, echo = FALSE}
TukeyHSD(anova_types)
```
:::

**Results:** There was a *statistically significant difference* in residuals between at least two of the hospital types (*F*(2, 203) = 25.96, *p* \< 0.001).

As we anticipated from @Fig-3,[^2] Tukeyâ€™s HSD Test for multiple comparisons *confirmed* that there was a statistically significant difference between both **nonprofit and government** types (*p* \< 0.001, 95% CI \[0.440, 1.071\]) and **nonprofit and neither** types (*p* \< 0.001, 95% CI \[0.536, 1.312\]) but no statistically significant difference between **government and neither** types (*p* = 0.650, 95% CI \[-0.614, 0.278\]).

[^2]: In the Additional Exploratory Analysis callout in Part A

These results suggest that nonprofit hospitals have different (higher) rates of technology adoption than government hospitals or hospitals that are neither, implying the possibility for government hospital technology improvement.

#### Part C

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
# linear regression model
lm_beds <- lm(saidin ~ beds, data = mma_2004)
summary(lm_beds)
confint(lm_beds)
```
:::

![Scatterplot of Saidin scores and beds in 2004](task1/figures/plot_beds_2004.png){#fig-5}

An initial scatterplot of Saidin scores and beds in 2004 (i.e., nonprofit, government, neither) visualizes a **seemingly positive correlation** between hospital size and technological capacity.

Simple linear regression analysis was used to test if the number of beds explained variation in Saidin index scores.

::: {.callout-note icon="false" appearance="simple"}
# Linear Regression Results

```{r, echo = FALSE}
# linear regression model
lm_beds <- lm(saidin ~ beds, data = mma_2004)
summary(lm_beds)
confint(lm_beds)
```
:::

**Results**: A *significant* regression was found (*F*(1, 1303) = 900.8, *p* \< 0.001). The R\^2 value of **0.408** indicates that the predictor (`beds`) explained about 40.8% of the variation in Saidin index scores. We can be 95% certain that the true change in Saidin scores as a function of the number of beds is between **0.00590 and 0.00673**, suggesting a positive correlation.

These results support the idea that the more beds a hospital has, the higher their Saidin scores tend to be, suggesting that there is a positive relationship between hospital capacity and technological availability. One possible non-causal explanation for these findings could be that hospital capacity indicates greater access to resources, which also leads to higher rates of technology adoption.

### Exercise 4

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
library(fixest)

# event study model
did_event <- 
  feols(saidin ~ i(year, treat, ref = 2004) | prov_id + year, data = mma_data)

# table of estimates and CI
did_estimates <- summary(did_event)
did_ci <- confint(did_event)

did_estimates_ci <- did_estimates$coefficients |>
  as.data.frame() |>
  tibble::rownames_to_column("year") |>
  left_join(
    as.data.frame(did_ci) |>
      tibble::rownames_to_column("year") |>
      rename(),
    by = join_by(year)
  ) |>
  janitor::clean_names() |>
  rename(
    tr_effect = did_estimates_coefficients,
    tr_hi = x97_5_percent,
    tr_lo = x2_5_percent
  ) |>
  mutate(year = as.numeric(str_extract(year, "\\d{4}")))

# table of means
did_means <- mma_data |>
  group_by(year) |>
  filter(treat == 0) |>
  summarize(mean(saidin)) |>
  rename(cr_mean = `mean(saidin)`) |>
  left_join(
    mma_data |>
      group_by(year) |>
      filter(treat == 1) |>
      summarize(mean(saidin)) |>
      rename(tr_mean = `mean(saidin)`)
  )

# join tables into new dataset
mma_did <- did_estimates_ci |>
  left_join(did_means)

```
:::

```{r, echo=FALSE}
library(gt)
load(file = here("task1/data/mma_did.rds"))
mma_did |> 
  gt(rowname_col = "year") |> 
  tab_header(
    title = md("Effect of MMA on adoption of medical technologies"), 
    subtitle = md("Estimates from <b style='color: #F8766D;'>fixest::feols()</b> with <b/>95% CI</b>")
  )
```

The `fixest::feols()` function was used to estimate a fixed effects linear regression model with clustered standard errors as part of a difference-in-difference analysis, to estimate the effect (`tr_effect`) of receiving a Section 508 waiver on the adoption of medical technologies with 95% confidence interval bounds (`tr_lo`, `tr_hi`).

Mean Saidin index scores were also calculated for the control group (`cr_mean`) and treatment group (`tr_mean`) in each year.

### Exercise 5

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
library(ggtext)

# 95 CI error bars
mma_test <- mma_did |>
  filter(year >= 2004) |>
  mutate(
    tr_ref = tr_mean - tr_effect,
    tr_ref_lo = tr_mean - tr_hi,
    tr_ref_hi = tr_mean - tr_lo
  ) |>
  full_join(
    mma_did |>
      filter(year < 2004)
  )

# visualization: significant effect
did_plot <- mma_test |>
  ggplot(aes(year)) +
  geom_line(aes(y = cr_mean), color = "#619CFF", alpha = 0.5) +
  geom_point(aes(y = cr_mean), color = "#619CFF") +
  geom_line(aes(y = tr_ref), color = "#00BA38", linetype = "dashed", alpha = 0.7) +
  geom_point(aes(y = tr_ref), color = "#00BA38") +
  geom_errorbar(aes(ymin = tr_ref_lo, ymax = tr_ref_hi), color = "#00BA38", width = 0.1) +
  geom_ribbon(aes(ymin = tr_ref_lo, ymax = tr_ref_hi), fill = "#00BA38", alpha = 0.1) +
  geom_point(aes(y = tr_mean), color = "#F8766D") +
  geom_line(aes(y = tr_mean), color = "#F8766D", alpha = 0.5) +
  geom_vline(xintercept = 2004, linetype = "dotted") +
  scale_x_continuous(n.breaks = 9) +
  scale_y_continuous(n.breaks = 6) +
  theme_minimal() +
  theme(
    panel.grid.minor.x = element_blank(),
    axis.title.x = element_blank(),
    plot.title.position = "plot",
    plot.title = element_markdown(),
    plot.subtitle = element_markdown()
  ) +
  labs(
    y = "Saidin Index Scores",
    title = "Technology adoption rates
    <b style='color: #000000;'>significantly increase </b><b style='color: #00BA38;'>before</b>
    and <b style='color: #F8766D;'>after</b> MMA."
  ) +
  annotate("text", x = 2008, y = 3.5, label = "control", color = "#619CFF", size = 3) +
  annotate("text", x = 2005.2, y = 6.4, label = "treatment", color = "#F8766D" , size = 3) +
  annotate("text", x = 2009.1, y = 7.4, label = "counterfactual", color = "#00BA38" , size = 2.7) +
  annotate("text", x = 2004, y = 8.5, label = "Year of\nAdoption", size = 3)
```

:::

![Difference-in-Differences Model](task1/figures/did_plot.png){#fig-6}

The difference-in-difference model suggests that Section 508 waivers resulted in a *significant increase in technology adoption rates* in hospitals. We see in @fig-6 that after accounting for treatment effects, the yearly means of the treatment group after the policy intervention in 2004 still **differ significantly** (95% CI) from the yearly means of the counterfactual group.

### Exercise 6
From the results of our difference-in-difference analysis, we can be 95% confident that MMA reimbursement rate adjustments had a positive impact on hospital technology adoption. As visualized in @fig-6, the yearly means of the treatment group still differed significantly from the counterfactual, or what would have been the treatment group without the effects of the policy intervention.

In our analysis we assume that:

1. in the absence of the policy intervention, the average outcomes of the treatment and control groups would have followed parallel paths over time;
2. there were no concurrent events or shocks occurring at the same time as the treatment that could have independently affected the outcome variable; and
3. the effect of the treatment did not become significantly stronger over time. 

In the event that these assumptions do not hold, the difference-in-differences estimator would no longer be valid, as the differences in the trends would not be able to be attributed to the treatment itself and the treatment alone.

In the case of the present analysis, we do see that:

1. the group means between the treatment and control groups before the year of adoption follow relatively parallel paths, with a slightly higher increase for the treatment group in 2003;
2. no major domestic health or economic policies were concurrently adopted in 2003 or 2004 beyond the Jobs and Growth Tax Relief Reconciliation Act, but the act did not particularly improve economic growth[^3]; and
3. acquiring new hospital technologies may have led to the acquisition of more, as new technologies open access to more patients and more efficient practices; but we actually see a slowing growth rate after 2007, suggesting that this effect may not be significant.

[^3]: [Center on Budget and Policy Priorities (2017)](https://www.cbpp.org/research/the-legacy-of-the-2001-and-2003-bush-tax-cuts#_ftn13)

## Task 2

### Exercise 1

::: {.callout-note collapse="true" icon="false"}
##  Code  
```{r, eval = FALSE}
library(tidyverse)
library(here)
library(tseries)

# Inspect data ----
skimr::skim(healthcare_data)

# confirm random auto-assignment
dummy_auto <- healthcare_data |>
  mutate(plan = ifelse(plan == "Plan_A", 1, 0)) |>
  filter(assignment_type == "Automatically Assigned")

runs.test(as.factor(dummy_auto$plan))

# test for random choice
dummy_choice <- healthcare_data |>
  mutate(plan = ifelse(plan == "Plan_A", 1, 0)) |>
  filter(assignment_type == "Member Choice")

runs.test(as.factor(dummy_choice$plan))

```

:::

The data was read in and all character variables were retyped as factors, resulting 21300 observations of 7 variables. An initial inspection of the data revealed no missingness, duplicates, or extreme imbalance between levels. 

A runs test completed using function `tseries::runs.test()` provides support for randomness in the plan assignment for automatically assigned Medicaid members. The test was non-significant at a 95% confidence level (*p* = **0.1999**), meaning that we do not reject the null hypothesis that the data is random. 

The same method was used to test for randomness in the plan assignment for Medicaid members who chose their plan. The test was also non-significant at a 95% confidence level (*p* = **0.709**), meaning that we do not reject the null hypothesis that the data is random. 

The higher p-value for the choice assignment versus automatic assignment could be due to differences in sample size. The larger the sample size, the more normal the distribution will be. Since the automatically assigned Medicaid members were the minority, there could have been more natural variation, resulting in the appearance of less randomness.

### Exercise 2

::: {.callout-note collapse="true" icon="false"}
##  Code  
```{r, eval = FALSE}
library(MatchIt)
library(marginaleffects)

# propensity score matching
psm_quick <- 
  matchit(
    plan ~ gender + assignment_type + health_status + language, 
    data = healthcare_data, 
    method = "quick",
    estimand = "ATE"
  )

# check balance
plot(psm_quick, type = "density", interactive = FALSE,
     which.xs = ~ assignment_type + health_status + language)

plot(summary(psm_quick))

# estimate ATE
psm_data <- match.data(psm_quick)

psm_fit <- glm(
  healthcare_spend ~ plan * (gender + assignment_type + health_status + language), 
  data = psm_data, weights = weights
)

avg_comparisons(
  psm_fit,
  variables = "plan",
  wts = "weights"
  )

```

:::

I chose to use propensity score matching to obtain a causal estimate of the managed care plan's effect on health care spending. My motivation was to match individuals in Plan A with individuals in Plan B who have similar observable characteristics (e.g., demographics) but differ in plan assignment. I could then attribute differences in healthcare spending between the matched individuals to the causal effect of plan assignment. 

In the analysis, Plan B was considered the treatment, and Plan A the control, such that the estimate would describe the difference in spending between Plan B - Plan A. 

I chose to use the general full matching method `quick` using the `MatchIt::matchit()` function with logistic regression, because we are looking to find the average treatment effect (ATE) of categorical variable in a large dataset. This matching specification yielded exact balance. In other words, the absolute standard mean difference between matched pairs was zero for all predictors (See @fig-7). Full matching uses all treated and all control units, so no units were discarded by the matching.

![Love plot before and after PSM](task2/figures/psm_plot.png){#fig-7}

To estimate the ATE and its standard error, I fit a logistic regression model using `glm()` with healthcare spending as the outcome and the plan "treatment", covariates, and their interaction as predictors, and used the full matching weights to make the estimation using the `marginaleffects::comparisons()` function. 

**Results**: The estimated effect was **-5.33** (*SE* = 1.34, *p* < 0.001, 95% CI [-7.95, -2.71]), indicating that the average effect of the treatment for those who received it is to decrease healthcare spending.

### Exercise 3

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
# simple average
aggregate(healthcare_spend ~ plan, data = healthcare_data, FUN = mean)

# regression model with fixed effects
lm_fit <- glm(healthcare_spend ~ plan + gender + health_status + language, data = healthcare_data)

summary(lm_fit)
```

:::

**Simple Average**: A straightforward way to begin to explore the causal effect of healthcare plan on spending would be to take simple average of spending for each plan group, where the effect estimate would be the difference between the two averages. The average of healthcare spending for Plan A was 190.6, as opposed to Plan B's average of 168.6. This difference (22.0) is almost four times larger than the effect estimate found through the PSM analysis. 

**Why the difference?** This discrepancy could be the result of confounding factors that are not taken into consideration in this simple average analysis, like the demographic variables, which may also directly impact spending and be distributed differently across each plan. For example, someone with a particular health condition might prefer one plan over another, but that health condition may also require significantly more spending than someone without that health condition.

**Regression**: A regression model can help to control for such demographic variables that may be correlated with both the choice of plan and healthcare spending. To obtain a causal estimate, I fitted a logistic regression model using `glm()` with healthcare spending as the outcome and the plan as the "treatment", with fixed effects for each of the demographics besides assignment type. The estimate this time was **-5.40** (*SE* = 1.362), significant at an alpha = 0.001 level. After considering for demographic variables, the causal estimate was much closer to the estimated effect from the PSM analysis (**-5.33**), as anticipated.

### Exercise 4

::: {.callout-note collapse="true" icon="false"}
##  Code  

```{r, eval = FALSE}
# modified psm_fit
psm_fit_fixed <- glm(
  healthcare_spend ~ plan + gender + health_status + language,
  data = psm_data, weights = weights
)

avg_comparisons(
  psm_fit_fixed,
  variables = "plan",
  wts = "weights"
)
```

::: 

Using fixed effects rather than the interaction terms in the causal model from Question 2 does not impact the causal estimate. Since both fixed effects and interaction terms aim to control for potential confounding factors in the causal model, the lack of impact could mean that the treatment effect is homogenous across different levels of the covariates. In other words, controlling for fixed effects to deal with the confounding may already adequately explain the variation in spending across plans. 

### Thank you for your time and consideration!
